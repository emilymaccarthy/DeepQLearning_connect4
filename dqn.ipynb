{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UG2oVzrU2usi"
      },
      "source": [
        "# DQN para Cart Pole\n",
        "\n",
        "**Tarea**\n",
        "\n",
        "El agente debe decidir entre dos acciones: mover el carro a la izquierda o a la derecha, de modo que el palo unido a él se mantenga en equilibrio. Para entender bien el funcionamiento del ambiente, es importante que **lean la siguiente página**: [sitio web de Gymnasium](https://gymnasium.farama.org/environments/classic_control/cart_pole/).\n",
        "\n",
        "![CartPole](https://pytorch.org/tutorials/_static/img/cartpole.gif)\n",
        "\n",
        "A medida que el agente observa el estado actual del ambiente y elige una acción, éste transiciona a un nuevo estado y también devuelve una recompensa que indica las consecuencias de la acción. En esta tarea, las recompensas son +1 por cada paso de tiempo incremental y el ambiente termina si el palo se inclina demasiado o si el carro se mueve más de 2.4 unidades del centro. Esto significa que los escenarios con mejor rendimiento durarán más tiempo, acumulando un retorno mayor.\n",
        "\n",
        "La tarea CartPole está diseñada para que las entradas al agente sean 4 valores de tipo **float** que representan el estado del ambiente (posición, velocidad, etc.). Tomamos estas 4 entradas sin ningún escalado y las pasamos por una pequeña red con 2 salidas, una para cada acción. La red se entrena para predecir el valor esperado para cada acción, dado el estado de entrada. Luego se elige la acción con el mayor valor esperado.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TEgWcDkf2usk"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import math\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple, deque\n",
        "from itertools import count\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lwS5WWT2usk"
      },
      "source": [
        "En el ambiente `CartPole-v1`, el **estado** está compuesto por 4 valores continuos:\n",
        "\n",
        "1. **Cart Position (posición del carro)**\n",
        "2. **Cart Velocity (velocidad del carro)**  \n",
        "3. **Pole Angle (ángulo del palo)**\n",
        "4. **Pole Angular Velocity (velocidad angular del palo):** rapidez con la que gira el palo.\n",
        "\n",
        "Es decir, el estado se representa como:\n",
        "\n",
        "state = [posición, velocidad, ángulo, velocidad\\_angular]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MaNeNKjw2usl"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"CartPole-v1\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8q08IeNy2usl",
        "outputId": "aff27f2a-e611-4eec-d2b8-c65f91239293"
      },
      "outputs": [],
      "source": [
        "state = env.reset()\n",
        "\n",
        "print(\"Estado inicial (vector):\", state)\n",
        "print(f\"Cart Position        : {state[0][0]}\")\n",
        "print(f\"Cart Velocity        : {state[0][1]}\")\n",
        "print(f\"Pole Angle           : {state[0][2]}\")\n",
        "print(f\"Pole Angular Velocity: {state[0][3]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iet-Am_72usl"
      },
      "outputs": [],
      "source": [
        "is_ipython = 'inline' in matplotlib.get_backend()\n",
        "if is_ipython:\n",
        "    from IPython import display\n",
        "\n",
        "plt.ion()\n",
        "\n",
        "device = torch.device(\n",
        "    \"cuda\" if torch.cuda.is_available() else\n",
        "    \"mps\" if torch.backends.mps.is_available() else\n",
        "    \"cpu\"\n",
        ")\n",
        "\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "env.reset(seed=seed)\n",
        "env.action_space.seed(seed)\n",
        "env.observation_space.seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(seed)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "455lYdLq2usl"
      },
      "source": [
        "# Componentes\n",
        "\n",
        "- Ambiente\n",
        "- Memoria\n",
        "- Red de política (Policy network)\n",
        "- Red objetivo (Target network)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgnxI1yr2usl"
      },
      "source": [
        "Memoria\n",
        "=====================\n",
        "\n",
        "Vamos a usar una memoria de experiencias para entrenar nuestra DQN. En ésta vamos a almacenar las transiciones que el agente observa, permitiéndonos reutilizar estos datos más adelante. Al muestrear aleatoriamente, las transiciones que conforman un batch no van a estar correlacionadas entre sí. Se ha demostrado que esto estabiliza y mejora en gran medida el procedimiento de entrenamiento de DQN.\n",
        "\n",
        "Para ello, vamos a necesitar dos clases:\n",
        "\n",
        "- `Transition` - una tupla que representa una única transición en nuestro ambiente. Esencialmente mapea pares (state, action) a su resultado (next_state, reward).\n",
        "- `ReplayMemory` - un búfer cíclico de tamaño acotado que contiene las transiciones observadas recientemente. También implementa un método `.sample()` para seleccionar un batch aleatorio de transiciones para el entrenamiento.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DhTW_SRA2usm"
      },
      "outputs": [],
      "source": [
        "Transition = namedtuple('Transition',\n",
        "                        ('state', 'action', 'next_state', 'reward'))\n",
        "\n",
        "\n",
        "class ReplayMemory(object):\n",
        "\n",
        "    def __init__(self, capacity): # TODO\n",
        "        \"\"\" Inicia la memoria como una lista vacía de capacidad maxima = capacity \"\"\"\n",
        "        pass\n",
        "\n",
        "    def push(self, *args):  # TODO\n",
        "        \"\"\" Guardar una transición \"\"\"\n",
        "        pass\n",
        "\n",
        "    def sample(self, batch_size): # TODO\n",
        "        \"\"\" Tomar una muestra aleatoria de la memoria de tamaño batch_size \"\"\"\n",
        "        pass\n",
        "\n",
        "    def __len__(self): # TODO\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3mhBZaM2usm"
      },
      "source": [
        "Ahora vamos a definir nuestro modelo. Pero antes, hagamos un repaso rápido de DQN.\n",
        "\n",
        "Algoritmo DQN\n",
        "=============\n",
        "\n",
        "Nuestro objetivo va a ser entrenar una política que trate de maximizar la recompensa acumulada y descontada\n",
        "$R_{t_0} = \\sum_{t=t_0}^{\\infty} \\gamma^{t - t_0} r_t$.\n",
        "\n",
        "El descuento, $\\gamma$, debe ser una constante entre $0$ y $1$ que asegure la convergencia de la suma. Un $\\gamma$ menor hace que las recompensas del futuro lejano incierto sean menos importantes para nuestro agente que las del futuro cercano, sobre las que puede estar más seguro. También incentiva a los agentes a recolectar recompensas más cercanas en el tiempo que recompensas equivalentes pero más lejanas.\n",
        "\n",
        "La idea principal detrás de Q-learning es que si tuviéramos una función\n",
        "$Q^*: State \\times Action \\rightarrow \\mathbb{R}$, que pudiera decirnos cuál sería nuestro retorno si tomáramos una acción en un estado dado, entonces podríamos construir fácilmente una política que maximice nuestras recompensas:\n",
        "\n",
        "$$\\pi^*(s) = \\arg\\!\\max_a \\ Q^*(s, a)$$\n",
        "\n",
        "Sin embargo, no sabemos todo sobre el ambiente, así que no tenemos acceso a $Q^*$. Pero, dado que las redes neuronales son aproximadores universales de funciones, podemos simplemente crear una y entrenarla para que se parezca a $Q^*$.\n",
        "\n",
        "Para nuestra regla de actualización de entrenamiento, usaremos el hecho de que toda función $Q$ para alguna política obedece la ecuación de Bellman:\n",
        "\n",
        "$$Q^{\\pi}(s, a) = r + \\gamma Q^{\\pi}(s', \\pi(s'))$$\n",
        "\n",
        "La diferencia entre ambos lados de la igualdad se conoce como el error de diferencia temporal, $\\delta$:\n",
        "\n",
        "$$\\delta = Q(s, a) - (r + \\gamma \\max_a' Q(s', a))$$\n",
        "\n",
        "Para minimizar este error, vamos a usar la [pérdida de Huber](https://en.wikipedia.org/wiki/Huber_loss). La pérdida de Huber actúa como el error cuadrático medio cuando el error es pequeño, pero como el error absoluto medio cuando el error es grande; esto la hace más robusta a valores atípicos cuando las estimaciones de $Q$ son muy ruidosas. Calculamos esto sobre un batch de transiciones, $B$, muestreado de la memoria de repetición:\n",
        "\n",
        "$$\\mathcal{L} = \\frac{1}{|B|}\\sum_{(s, a, s', r) \\ \\in \\ B} \\mathcal{L}(\\delta)$$\n",
        "\n",
        "$$\\begin{aligned}\n",
        "\\text{donde} \\quad \\mathcal{L}(\\delta) = \\begin{cases}\n",
        "  \\frac{1}{2}{\\delta^2}  & \\text{si } |\\delta| \\le 1, \\\\\n",
        "  |\\delta| - \\frac{1}{2} & \\text{en otro caso.}\n",
        "\\end{cases}\n",
        "\\end{aligned}$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Mb-R1y22usm"
      },
      "source": [
        "Q-network\n",
        "-----------------\n",
        "\n",
        "Nuestro modelo será una red neuronal feed-forward que recibe un vector que representa a un estado. Tiene dos salidas, que representan $Q(s, \\mathrm{left})$ y $Q(s, \\mathrm{right})$ (donde $s$ es la entrada de la red). En efecto, la red intenta predecir el retorno esperado de tomar cada acción dada la entrada actual.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UMJ_gSvK2usm"
      },
      "outputs": [],
      "source": [
        "class DQN(nn.Module):\n",
        "\n",
        "    def __init__(self, n_observations, n_actions): # TODO\n",
        "        \"\"\" Definir la arquitectura de la red FF \"\"\"\n",
        "        super(DQN, self).__init__()\n",
        "        pass\n",
        "\n",
        "    def forward(self, x): # TODO\n",
        "        \"\"\" Definir las funciones de activación de la red \"\"\"\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z45j2uBS2usm"
      },
      "source": [
        "Entrenamiento\n",
        "=============\n",
        "\n",
        "Hiperparámetros y utilidades\n",
        "----------------------------\n",
        "\n",
        "Vamos a instanciar nuestro modelo y su optimizador, y definir algunas utilidades:\n",
        "\n",
        "- `select_action` - seleccionará una acción de acuerdo con una política e-greedy. En pocas palabras, a veces usaremos nuestro modelo para elegir la acción, y otras veces simplemente muestrearemos una uniformemente. La probabilidad de elegir una acción aleatoria comenzará en `EPS_START` y decaerá exponencialmente hacia `EPS_END`. `EPS_DECAY` controla la tasa de decaimiento.\n",
        "- `plot_durations` - una función auxiliar para graficar la duración de los episodios, junto con un promedio sobre los últimos 100 episodios (la medida usada en las evaluaciones oficiales). La gráfica estará debajo de la celda que contiene el bucle principal de entrenamiento y se actualizará después de cada episodio.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RhbFIpJi2usn"
      },
      "outputs": [],
      "source": [
        "# BATCH_SIZE es el número de transiciones muestreadas de la memoria\n",
        "# GAMMA es el factor de descuento mencionado en la sección anterior\n",
        "# EPS_START es el valor inicial de epsilon\n",
        "# EPS_END es el valor final de epsilon\n",
        "# EPS_DECAY controla la tasa de decaimiento exponencial de epsilon; valores altos implican un decaimiento más lento\n",
        "# TAU es la tasa de actualización de la red objetivo\n",
        "# LR es la tasa de aprendizaje del optimizador AdamW\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "GAMMA = 0.99\n",
        "EPS_START = 0.9\n",
        "EPS_END = 0.01\n",
        "EPS_DECAY = 2500\n",
        "TAU = 0.005\n",
        "LR = 3e-4\n",
        "\n",
        "# Obtenemos el número de acciones del espacio de acciones\n",
        "n_actions = env.action_space.n\n",
        "# Obtenemos el número de observaciones de estado\n",
        "state, info = env.reset()\n",
        "n_observations = len(state)\n",
        "\n",
        "policy_net = DQN(n_observations, n_actions).to(device)\n",
        "target_net = DQN(n_observations, n_actions).to(device)\n",
        "# Copiamos los pesos de las redes \n",
        "target_net.load_state_dict(policy_net.state_dict()) \n",
        "\n",
        "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
        "memory = ReplayMemory(10000)\n",
        "\n",
        "steps_done = 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RPBqsVZ02usn"
      },
      "outputs": [],
      "source": [
        "def select_action(state): \n",
        "    global steps_done\n",
        "    sample = random.random()\n",
        "    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)\n",
        "    steps_done += 1\n",
        "    if sample > eps_threshold:\n",
        "        with torch.no_grad():\n",
        "            return policy_net(state).max(1).indices.view(1, 1)\n",
        "    else:\n",
        "        return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lR8JbWqB2usn"
      },
      "outputs": [],
      "source": [
        "episode_durations = []\n",
        "\n",
        "def plot_durations(show_result=False):\n",
        "    plt.figure(1)\n",
        "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
        "    if show_result:\n",
        "        plt.title('Resultado')\n",
        "    else:\n",
        "        plt.clf()\n",
        "        plt.title('Entrenando...')\n",
        "    plt.xlabel('Episodio')\n",
        "    plt.ylabel('Duración')\n",
        "    plt.plot(durations_t.numpy())\n",
        "    # Tomar promedios de 100 episodios y graficarlos también\n",
        "    if len(durations_t) >= 100:\n",
        "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
        "        means = torch.cat((torch.zeros(99), means))\n",
        "        plt.plot(means.numpy())\n",
        "\n",
        "    plt.pause(0.001)\n",
        "    if is_ipython:\n",
        "        if not show_result:\n",
        "            display.display(plt.gcf())\n",
        "            display.clear_output(wait=True)\n",
        "        else:\n",
        "            display.display(plt.gcf())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuHv3Vo42usn"
      },
      "source": [
        "Bucle de entrenamiento\n",
        "======================\n",
        "\n",
        "Finalmente, el código para entrenar nuestro modelo.\n",
        "\n",
        "La función `optimize_model` realiza un único paso de optimización. Primero muestrea un batch, concatena todos los tensores en uno solo, calcula $Q(s_t, a_t)$ y $V(s_{t+1}) = \\max_a Q(s_{t+1}, a)$, y los combina en nuestra pérdida. Por definición fijamos $V(s) = 0$ si $s$ es un estado terminal. También usamos una red objetivo para calcular $V(s_{t+1})$ para mayor estabilidad. La red objetivo se actualiza en cada paso con una [actualización suave](https://arxiv.org/pdf/1509.02971.pdf) controlada por el hiperparámetro `TAU`, que definimos previamente.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fW7eESoR2usn"
      },
      "outputs": [],
      "source": [
        "def optimize_model():\n",
        "    if len(memory) < BATCH_SIZE:\n",
        "        return\n",
        "    transitions = memory.sample(BATCH_SIZE)\n",
        "    # Transponer el lote (ver https://stackoverflow.com/a/19343/3343043 para\n",
        "    # una explicación detallada). Esto convierte un arreglo de Transitions por lote\n",
        "    # a un Transition de arreglos por lote.\n",
        "    batch = Transition(*zip(*transitions))\n",
        "\n",
        "    # Calcular una máscara de estados no finales y concatenar los elementos del lote\n",
        "    # (un estado final sería aquel después del cual la simulación terminó)\n",
        "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
        "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
        "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
        "                                                if s is not None])\n",
        "    state_batch = torch.cat(batch.state)\n",
        "    action_batch = torch.cat(batch.action)\n",
        "    reward_batch = torch.cat(batch.reward)\n",
        "\n",
        "    # Calcular Q(s_t, a) - el modelo calcula Q(s_t), luego seleccionamos las\n",
        "    # columnas de las acciones tomadas. Estas son las acciones que se habrían tomado\n",
        "    # para cada estado del lote según policy_net\n",
        "    state_action_values = # TODO\n",
        "\n",
        "    # Calcular V(s_{t+1}) para todos los estados siguientes.\n",
        "    # Los valores esperados de las acciones para non_final_next_states se calculan\n",
        "    # con la target_net \"más antigua\"; seleccionando su mejor recompensa\n",
        "    # Esto se combina según la máscara, de modo que tengamos el valor de estado esperado\n",
        "    # o 0 en caso de que el estado sea final.\n",
        "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
        "    with torch.no_grad():\n",
        "        next_state_values[non_final_mask] = # TODO\n",
        "    # Calcular los valores Q esperados\n",
        "    expected_state_action_values = # TODO\n",
        "\n",
        "    # Calcular pérdida de Huber\n",
        "    criterion = nn.SmoothL1Loss()\n",
        "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
        "\n",
        "    # Optimizar el modelo\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    # Recorte de gradientes in-place\n",
        "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
        "    optimizer.step()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hj-U72RA2uso"
      },
      "source": [
        "A continuación, vemos como quedaría bucle principal de entrenamiento. Al principio reiniciamos el ambiente y obtenemos el `state` inicial como un Tensor. Luego, muestreamos una acción, la ejecutamos, observamos el siguiente estado y la recompensa (siempre 1), y optimizamos nuestro modelo una vez. Cuando termina el episodio (nuestro modelo falla), reiniciamos el bucle.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 526
        },
        "id": "E5R-Lypo2uso",
        "outputId": "bd5b694c-7790-45ce-ff28-37eedc4888bd"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available() or torch.backends.mps.is_available():\n",
        "    num_episodes = 600\n",
        "else:\n",
        "    num_episodes = 50\n",
        "\n",
        "for i_episode in range(num_episodes):\n",
        "    # Inicializar el ambiente y obtener su estado\n",
        "    state, info = env.reset()\n",
        "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "    for t in count():\n",
        "        action = # TODO\n",
        "        observation, reward, terminated, truncated, _ = env.step(action.item())\n",
        "        reward = torch.tensor([reward], device=device)\n",
        "        done = terminated or truncated\n",
        "\n",
        "        if terminated:\n",
        "            next_state = None\n",
        "        else:\n",
        "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "\n",
        "        # TODO: Guardar la transición en la memoria\n",
        "\n",
        "        # TODO: Moverse al siguiente estado\n",
        "    \n",
        "        # Realizar un paso de optimización (en la red de política)\n",
        "        optimize_model()\n",
        "\n",
        "        # Actualización suave de los pesos de la red objetivo\n",
        "        # θ′ ← τ θ + (1 −τ )θ′\n",
        "        target_net_state_dict = target_net.state_dict()\n",
        "        policy_net_state_dict = policy_net.state_dict()\n",
        "        for key in policy_net_state_dict:\n",
        "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
        "        target_net.load_state_dict(target_net_state_dict)\n",
        "\n",
        "        if done:\n",
        "            episode_durations.append(t + 1)\n",
        "            plot_durations()\n",
        "            break\n",
        "\n",
        "print('Completado')\n",
        "plot_durations(show_result=True)\n",
        "plt.ioff()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAQXlGu52uso"
      },
      "source": [
        "Diagrama explicativo del flujo de datos\n",
        "\n",
        "![](https://pytorch.org/tutorials/_static/img/reinforcement_learning_diagram.jpg)\n",
        "\n",
        "Las acciones se eligen aleatoriamente o en base a una política, obteniendo la siguiente muestra del ambiente de gym. Registramos los resultados en la memoria de repetición y también ejecutamos un paso de optimización en cada iteración. La optimización elige un batch aleatorio de la memoria de repetición para entrenar la nueva política. La `target_net` \"más antigua\" también se usa en la optimización para calcular los valores Q esperados. Se realiza una actualización de los pesos en cada paso del entrenamiento.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
